# ğŸ“Œ Lecture 6 - Containers: Docker & Kubernetes

## ğŸ“ Slide 1 â€“ ğŸ³ What are Containers?

* ğŸ“¦ **Container** = a lightweight, standalone, executable package that includes everything needed to run a piece of software (code, runtime, system tools, libraries, settings).
* ğŸ§© **Key Concept**: Containers package an application and its dependencies together, ensuring it runs consistently across different computing environments.
* ğŸ—ï¸ **Isolation without full virtualization**: Containers share the host operating system kernel but run in isolated user spaces.
* âš¡ **Benefits**: Fast startup (milliseconds), minimal resource overhead, high density (hundreds of containers on one host).
* ğŸ¯ **Simple Analogy**: Like shipping containers ğŸ“¦ - standardized, stackable, portable; you don't care what's inside, just that it fits the standard.

**Container Architecture Overview**
```mermaid
flowchart LR
    subgraph "ğŸ–¥ï¸ Physical Host"
        kernel["ğŸ§  Linux Kernel<br/>Shared by all containers"]
        
        subgraph "ğŸ³ Container Runtime"
            engine["âš™ï¸ Container Engine<br/>Docker/Podman/containerd"]
        end
        
        subgraph "ğŸ“¦ Containers"
            c1["ğŸ“¦ Container 1<br/>App A + Libs<br/>Isolated process"]
            c2["ğŸ“¦ Container 2<br/>App B + Libs<br/>Isolated process"]
            c3["ğŸ“¦ Container 3<br/>App C + Libs<br/>Isolated process"]
        end
    end
    
    kernel --> engine
    engine --> c1
    engine --> c2
    engine --> c3
    
    style kernel fill:#e8f4f8,stroke:#2c3e50,color:#2c3e50
    style engine fill:#fff4e6,stroke:#2c3e50,color:#2c3e50
    style c1 fill:#e8f8f5,stroke:#2c3e50,color:#2c3e50
    style c2 fill:#fef5e7,stroke:#2c3e50,color:#2c3e50
    style c3 fill:#f4ecf7,stroke:#2c3e50,color:#2c3e50
```

* ğŸ”‘ **Core Components**:
  * ğŸ“¸ **Image** = read-only template with application code and dependencies
  * ğŸ“¦ **Container** = running instance of an image
  * ğŸ—„ï¸ **Registry** = storage and distribution system for images (Docker Hub, Quay.io, GHCR = GitHub Container Registry)
* ğŸ’¡ **Real-World Impact**: Netflix runs hundreds of thousands of containers, Spotify deploys 10,000+ containers daily
* ğŸŒ **Industry Adoption**: 87% of organizations use containers in production (CNCF = Cloud Native Computing Foundation Survey 2024)

ğŸ”— **Resources:**
* [Docker: What is a Container?](https://www.docker.com/resources/what-container/)
* [CNCF Cloud Native Glossary - Container](https://glossary.cncf.io/container/)
* [Linux Containers (LXC) Overview](https://linuxcontainers.org/)

---

## ğŸ“ Slide 2 â€“ ğŸ“œ History of Containers (1979 â†’ 2024)

* ğŸ›ï¸ **1979 - The Beginning**: Unix V7 introduces `chroot` (change root) - isolates file system view for processes
* ğŸ“¦ **2000 - FreeBSD Jails**: First "true" container technology with process and network isolation
* ğŸ§ **2001 - Linux VServer**: Partition resources on Linux, early resource isolation
* ğŸ“¦ **2004 - Solaris Zones**: Oracle/Sun's enterprise container solution
* ğŸ”§ **2006 - Process Containers**: Google creates cgroups (control groups) for Linux kernel
* ğŸ“¦ **2008 - LXC (Linux Containers)**: First complete Linux container manager using cgroups + namespaces
* ğŸ³ **2013 - Docker Revolution**: Solomon Hykes releases Docker, makes containers accessible to everyone
* â˜¸ï¸ **2014 - Kubernetes Born**: Google open-sources Kubernetes based on Borg/Omega experience
* ğŸ¯ **2015 - OCI (Open Container Initiative)**: Industry standardizes container formats and runtimes
* ğŸš€ **2016-2024 - Cloud Native Era**: Containers become the standard for cloud deployments

**Evolution Timeline**
```mermaid
flowchart LR
    A["ğŸ›ï¸ 1979<br/>Unix chroot"] --> B["ğŸ“¦ 2000<br/>FreeBSD Jails"]
    B --> C["ğŸ§ 2008<br/>LXC Linux Containers"]
    C --> D["ğŸ³ 2013<br/>Docker Released"]
    D --> E["â˜¸ï¸ 2014<br/>Kubernetes"]
    E --> F["ğŸŒ 2015<br/>OCI Standard"]
    F --> G["ğŸš€ 2024<br/>Cloud Native<br/>Mainstream"]
    
    style A fill:#f9e79f,stroke:#2c3e50,color:#2c3e50
    style B fill:#aed6f1,stroke:#2c3e50,color:#2c3e50
    style C fill:#a9dfbf,stroke:#2c3e50,color:#2c3e50
    style D fill:#f5b7b1,stroke:#2c3e50,color:#2c3e50
    style E fill:#d7bde2,stroke:#2c3e50,color:#2c3e50
    style F fill:#aed6f1,stroke:#2c3e50,color:#2c3e50
    style G fill:#a9dfbf,stroke:#2c3e50,color:#2c3e50
```

* ğŸ­ **Fun Historical Facts**:
  * ğŸ’¡ Docker was originally a PaaS (Platform as a Service) company called dotCloud; containers were their internal tool
  * ğŸ³ The Docker whale logo carries shipping containers because of the shipping container analogy
  * â˜¸ï¸ "Kubernetes" (ÎºÏ…Î²ÎµÏÎ½Î®Ï„Î·Ï‚) is Greek for "helmsman" or "pilot" - hence the ship wheel logo
  * ğŸ† Google was running 2+ billion containers per week in 2014 (before Kubernetes public release)
  * ğŸ“¦ The first container in Docker Hub was "hello-world" (still exists today!)
* ğŸ”„ **Key Paradigm Shifts**:
  * **2000s**: Containers were complex, expert-only tools
  * **2013-2015**: Docker made containers developer-friendly
  * **2016-2020**: Kubernetes standardized orchestration
  * **2020+**: Containers are now default for cloud-native applications

ğŸ”— **Resources:**
* [Docker History Timeline](https://www.docker.com/company/)
* [Kubernetes Origin Story](https://kubernetes.io/blog/2015/04/borg-predecessor-to-kubernetes/)
* [A Brief History of Containers](https://blog.aquasec.com/a-brief-history-of-containers-from-1970s-chroot-to-docker-2016)
* [OCI Announcement](https://www.opencontainers.org/about/overview/)

---

## ğŸ“ Slide 3 â€“ ğŸ’¡ Why Containers Matter in DevOps

* âš¡ **Speed**: Applications start in milliseconds vs minutes for VMs (Virtual Machines)
* ğŸ”„ **Consistency**: "Works on my machine" problem solved - same container runs everywhere
* ğŸ“¦ **Portability**: Run the same container on laptop, data center, any cloud provider
* ğŸ¯ **Efficiency**: 10-100x higher density than VMs, better resource utilization
* ğŸš€ **Scalability**: Spin up/down instances in seconds to handle traffic spikes
* ğŸ§ª **Development Speed**: Developers can replicate production environments locally

**Problems Containers Solve**
```mermaid
flowchart LR
    subgraph "âŒ Before Containers"
        P1["ğŸ› Works on my machine<br/>Environment inconsistency"]
        P2["ğŸŒ Slow deployments<br/>Manual setup"]
        P3["ğŸ’° Resource waste<br/>Underutilized servers"]
        P4["ğŸ”¥ Dependency hell<br/>Conflicting versions"]
    end
    
    subgraph "âœ… After Containers"
        S1["ğŸ¯ Consistent everywhere<br/>Package dependencies"]
        S2["âš¡ Deploy in seconds<br/>Automated & repeatable"]
        S3["ğŸ“Š High density<br/>100+ containers/host"]
        S4["ğŸ§© Isolated environments<br/>No conflicts"]
    end
    
    P1 --> S1
    P2 --> S2
    P3 --> S3
    P4 --> S4
    
    style P1 fill:#fadbd8,stroke:#2c3e50,color:#2c3e50
    style P2 fill:#fadbd8,stroke:#2c3e50,color:#2c3e50
    style P3 fill:#fadbd8,stroke:#2c3e50,color:#2c3e50
    style P4 fill:#fadbd8,stroke:#2c3e50,color:#2c3e50
    style S1 fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
    style S2 fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
    style S3 fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
    style S4 fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
```

* ğŸ“Š **Real-World Impact Statistics**:
  * âš¡ **Deployment Speed**: Netflix reduced deployment time from 45 minutes to under 60 seconds
  * ğŸ’° **Cost Savings**: Airbnb cut infrastructure costs by 40% after containerization
  * ğŸ“ˆ **Scale**: Twitter runs 100,000+ containers across their infrastructure
  * ğŸš€ **Developer Productivity**: GitHub reduced onboarding time from days to hours
  * ğŸ”„ **Release Frequency**: Companies deploying containers ship code 200x more frequently
* ğŸ¯ **DevOps Workflow Benefits**:
  * **ğŸ§ª Testing**: Identical environments for dev, staging, production
  * **ğŸ”„ CI/CD (Continuous Integration/Continuous Deployment)**: Fast, repeatable builds
  * **ğŸŒ Multi-cloud**: Avoid vendor lock-in, run anywhere
  * **ğŸ”§ Microservices**: Natural fit for distributed architectures
  * **ğŸ“¦ Dependency Management**: Bundle everything needed, no host conflicts

**Container-Driven DevOps Pipeline**
```mermaid
flowchart LR
    Dev["ğŸ‘¨â€ğŸ’» Developer"] --> Code["ğŸ“ Write Code<br/>+ Dockerfile"]
    Code --> Build["ğŸ—ï¸ Build Image<br/>CI Pipeline"]
    Build --> Test["ğŸ§ª Test Container<br/>Automated tests"]
    Test --> Registry["ğŸ—„ï¸ Push to Registry<br/>Docker Hub/GHCR"]
    Registry --> Deploy["ğŸš€ Deploy<br/>Dev/Staging/Prod"]
    Deploy --> Monitor["ğŸ“Š Monitor<br/>Logs & metrics"]
    Monitor --> Dev
    
    style Dev fill:#e8f8f5,stroke:#2c3e50,color:#2c3e50
    style Code fill:#fef5e7,stroke:#2c3e50,color:#2c3e50
    style Build fill:#e8f4f8,stroke:#2c3e50,color:#2c3e50
    style Test fill:#f4ecf7,stroke:#2c3e50,color:#2c3e50
    style Registry fill:#fdebd0,stroke:#2c3e50,color:#2c3e50
    style Deploy fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
    style Monitor fill:#fadbd8,stroke:#2c3e50,color:#2c3e50
```

ğŸ”— **Resources:**
* [Why Containers? (Red Hat)](https://www.redhat.com/en/topics/containers)
* [Container Benefits (AWS)](https://aws.amazon.com/containers/)
* [State of Container Adoption Report (Datadog)](https://www.datadoghq.com/container-report/)

---

## ğŸ“ Slide 4 â€“ âš–ï¸ Containers vs Virtual Machines (VMs)

* ğŸ—ï¸ **Architecture Difference**: VMs virtualize hardware, containers virtualize the operating system
* âš¡ **Startup Time**: Containers = milliseconds, VMs = minutes
* ğŸ’¾ **Size**: Container images = MBs (megabytes), VM images = GBs (gigabytes)
* ğŸ§  **Resource Overhead**: Containers ~1-5% overhead, VMs ~10-20% overhead
* ğŸ”’ **Isolation Level**: VMs = stronger (separate kernels), Containers = good (shared kernel, namespace isolation)
* ğŸ¯ **Best Use**: VMs for different OSes/strong isolation, Containers for microservices/scalability

**Side-by-Side Comparison**
```mermaid
flowchart TD
    subgraph "ğŸ–¥ï¸ Virtual Machine Architecture"
        VM_Apps["ğŸ“± App A | ğŸ“± App B | ğŸ“± App C"]
        VM_Guest["ğŸ–¥ï¸ Guest OS | ğŸ–¥ï¸ Guest OS | ğŸ–¥ï¸ Guest OS"]
        VM_Hyper["âš™ï¸ Hypervisor<br/>ESXi, Hyper-V, KVM"]
        VM_Host["ğŸ’» Host Operating System"]
        VM_HW["ğŸ–¥ï¸ Physical Hardware"]
        
        VM_Apps --> VM_Guest
        VM_Guest --> VM_Hyper
        VM_Hyper --> VM_Host
        VM_Host --> VM_HW
    end
    
    subgraph "ğŸ³ Container Architecture"
        C_Apps["ğŸ“± App A | ğŸ“± App B | ğŸ“± App C"]
        C_Runtime["ğŸ³ Container Runtime<br/>Docker, Podman, containerd"]
        C_Host["ğŸ’» Host Operating System"]
        C_HW["ğŸ–¥ï¸ Physical Hardware"]
        
        C_Apps --> C_Runtime
        C_Runtime --> C_Host
        C_Host --> C_HW
    end
    
    style VM_Apps fill:#fdebd0,stroke:#2c3e50,color:#2c3e50
    style VM_Guest fill:#fadbd8,stroke:#2c3e50,color:#2c3e50
    style VM_Hyper fill:#e8f4f8,stroke:#2c3e50,color:#2c3e50
    style C_Apps fill:#fdebd0,stroke:#2c3e50,color:#2c3e50
    style C_Runtime fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
```

* ğŸ“Š **Performance Comparison Table**:

| **Aspect** | **ğŸ³ Containers** | **ğŸ’» Virtual Machines** |
|------------|------------------|------------------------|
| **Startup Time** | âš¡ 50-500ms | ğŸŒ 1-5 minutes |
| **Disk Space** | ğŸ“¦ 10-200 MB | ğŸ’¿ 2-20 GB |
| **Memory Usage** | ğŸ’¾ MBs per container | ğŸ’¾ GBs per VM |
| **Performance** | ğŸš€ Near-native | ğŸ“‰ 5-20% overhead |
| **Density** | ğŸ“Š 100+ per host | ğŸ“Š 10-20 per host |
| **Boot Process** | âš¡ Process start | ğŸ”„ Full OS boot |
| **Isolation** | ğŸ§© Process-level | ğŸ”’ Hardware-level |
| **Portability** | ğŸŒ Highly portable | ğŸŒ Less portable |

* ğŸ¯ **When to Choose What**:
  * **ğŸ³ Choose Containers For**:
    * ğŸ”— Microservices architectures
    * âš¡ Rapid scaling requirements
    * ğŸš€ CI/CD pipelines
    * ğŸŒ Multi-cloud deployments
    * ğŸ’° Cost-sensitive workloads
  * **ğŸ’» Choose VMs For**:
    * ğŸ–¥ï¸ Different OS requirements (Windows + Linux)
    * ğŸ”’ Strong security isolation needs
    * ğŸ¢ Legacy monolithic applications
    * ğŸ›¡ï¸ Compliance/regulatory requirements
    * ğŸ® Workloads needing specific kernels

* ğŸ”„ **Hybrid Approach - Best of Both Worlds**:
  * ğŸ—ï¸ **Containers on VMs**: Most common pattern - run containers inside VMs for additional isolation layer
  * â˜¸ï¸ **Kubernetes Nodes**: VMs running container orchestration platforms
  * ğŸŒ **Cloud Providers**: AWS Fargate, Google Cloud Run use VMs to isolate customer containers

**Hybrid Architecture Example**
```mermaid
flowchart LR
    Cloud["â˜ï¸ Cloud Provider"] --> VM1["ğŸ’» VM 1<br/>Kubernetes Node"]
    Cloud --> VM2["ğŸ’» VM 2<br/>Kubernetes Node"]
    Cloud --> VM3["ğŸ’» VM 3<br/>Kubernetes Node"]
    
    VM1 --> C1["ğŸ³ Container A"]
    VM1 --> C2["ğŸ³ Container B"]
    VM2 --> C3["ğŸ³ Container C"]
    VM2 --> C4["ğŸ³ Container D"]
    VM3 --> C5["ğŸ³ Container E"]
    VM3 --> C6["ğŸ³ Container F"]
    
    style Cloud fill:#e8f4f8,stroke:#2c3e50,color:#2c3e50
    style VM1 fill:#fef5e7,stroke:#2c3e50,color:#2c3e50
    style VM2 fill:#fef5e7,stroke:#2c3e50,color:#2c3e50
    style VM3 fill:#fef5e7,stroke:#2c3e50,color:#2c3e50
    style C1 fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
    style C2 fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
    style C3 fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
    style C4 fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
    style C5 fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
    style C6 fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
```

ğŸ”— **Resources:**
* [Containers vs VMs (Docker)](https://www.docker.com/resources/what-container/)
* [Understanding the Differences (Red Hat)](https://www.redhat.com/en/topics/containers/containers-vs-vms)
* [Container Performance Study (IBM Research)](https://domino.research.ibm.com/library/cyberdig.nsf/papers/0929052195DD819C85257D2300681E7B)

---

## ğŸ“ Slide 5 â€“ ğŸ§© Container Components (Images, Layers, Registries)

* ğŸ“¸ **Container Image** = read-only template containing application code, runtime, libraries, dependencies, and configuration
* ğŸ“¦ **Container** = running instance of an image (process with isolated namespace)
* ğŸ—„ï¸ **Registry** = centralized repository for storing and distributing container images
* ğŸ‚ **Layers** = images are built from layers stacked on top of each other (copy-on-write filesystem)
* ğŸ·ï¸ **Tags** = version labels for images (e.g., `nginx:1.25`, `nginx:latest`)

**Container Image Anatomy**
```mermaid
flowchart LR
    subgraph "ğŸ“¸ Container Image"
        Base["ğŸ§± Base Layer<br/>Alpine Linux 3.18<br/>5 MB"]
        Runtime["âš™ï¸ Runtime Layer<br/>Python 3.11<br/>50 MB"]
        Deps["ğŸ“¦ Dependencies Layer<br/>pip packages<br/>100 MB"]
        App["ğŸ“± Application Layer<br/>Your code<br/>10 MB"]
        Config["âš™ï¸ Config Layer<br/>ENV vars, CMD<br/>1 KB"]
    end
    
    Base --> Runtime
    Runtime --> Deps
    Deps --> App
    App --> Config
    
    Config --> Running["ğŸš€ Running Container<br/>Process with isolated<br/>namespaces"]
    
    style Base fill:#e8f4f8,stroke:#2c3e50,color:#2c3e50
    style Runtime fill:#fef5e7,stroke:#2c3e50,color:#2c3e50
    style Deps fill:#f4ecf7,stroke:#2c3e50,color:#2c3e50
    style App fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
    style Config fill:#fdebd0,stroke:#2c3e50,color:#2c3e50
    style Running fill:#a9dfbf,stroke:#2c3e50,color:#2c3e50
```

* ğŸ‚ **Layer System Explained**:
  * ğŸ§± **Base Layer**: Usually minimal OS (Alpine, Ubuntu, scratch)
  * âš™ï¸ **Intermediate Layers**: Runtime, system dependencies
  * ğŸ“¦ **Dependency Layers**: Application dependencies
  * ğŸ“± **Application Layer**: Your source code
  * âš™ï¸ **Config Layer**: Metadata (ENV, CMD, ENTRYPOINT, EXPOSE)
* ğŸ’¡ **Layer Benefits**:
  * ğŸ”„ **Reusability**: Shared layers across images save disk space
  * âš¡ **Fast Builds**: Cached layers speed up rebuilds
  * ğŸ“¦ **Efficient Distribution**: Only changed layers transferred
  * ğŸ’¾ **Storage Optimization**: Copy-on-write filesystem

**Image to Container Lifecycle**
```mermaid
flowchart LR
    Dockerfile["ğŸ“ Dockerfile<br/>Build instructions"] --> Build["ğŸ—ï¸ docker build"]
    Build --> Image["ğŸ“¸ Image<br/>myapp:1.0<br/>Read-only"]
    Image --> Registry["ğŸ—„ï¸ Registry<br/>Docker Hub<br/>Quay.io<br/>GHCR"]
    Registry --> Pull["ğŸ“¥ docker pull"]
    Pull --> Run["ğŸš€ docker run"]
    Run --> Container["ğŸ“¦ Container<br/>Running process<br/>Writable layer"]
    
    style Dockerfile fill:#fef5e7,stroke:#2c3e50,color:#2c3e50
    style Build fill:#e8f4f8,stroke:#2c3e50,color:#2c3e50
    style Image fill:#f4ecf7,stroke:#2c3e50,color:#2c3e50
    style Registry fill:#fdebd0,stroke:#2c3e50,color:#2c3e50
    style Pull fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
    style Run fill:#a9dfbf,stroke:#2c3e50,color:#2c3e50
    style Container fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
```

* ğŸ—„ï¸ **Popular Container Registries**:
  * ğŸ³ **Docker Hub**: Public and private images (hub.docker.com)
  * ğŸ“¦ **Quay.io**: Red Hat's registry with security scanning
  * ğŸ™ **GHCR (GitHub Container Registry)**: Integrated with GitHub repos
  * ğŸŒ **AWS ECR (Elastic Container Registry)**: Amazon's private registry
  * â˜ï¸ **Google Artifact Registry**: Google Cloud's unified repository
  * ğŸ”· **Azure Container Registry**: Microsoft's registry service

* ğŸ·ï¸ **Image Naming Convention**:
  * ğŸ“ Format: `[registry/][namespace/]repository[:tag]`
  * ğŸŒ Examples:
    * `nginx` â†’ Docker Hub official image
    * `nginx:1.25` â†’ Specific version
    * `nginx:alpine` â†’ Alpine Linux variant
    * `ghcr.io/user/app:latest` â†’ GitHub Container Registry
    * `myregistry.com:5000/team/api:v2.1.0` â†’ Private registry

ğŸ”— **Resources:**
* [Docker Image Specification](https://github.com/moby/moby/blob/master/image/spec/v1.2.md)
* [Docker Hub Registry](https://hub.docker.com/)
* [Understanding Container Images (Red Hat)](https://www.redhat.com/en/topics/containers/what-is-a-container-image)
* [Image Layer Visualization](https://github.com/wagoodman/dive)

---

## ğŸ“ Slide 6 â€“ ğŸ› ï¸ Container Engine Differences (Docker vs Podman vs containerd vs CRI-O)

* ğŸ³ **Docker** = Complete platform with daemon, CLI, build tools, networking, orchestration (swarm)
* ğŸ¦­ **Podman** = Daemonless, rootless alternative to Docker; CLI-compatible, no central daemon
* âš™ï¸ **containerd** = Industry-standard container runtime, core component of Docker and Kubernetes
* ğŸ¯ **CRI-O (Container Runtime Interface - Open Container Initiative)** = Lightweight runtime designed specifically for Kubernetes

**Container Engine Ecosystem**
```mermaid
flowchart TD
    subgraph "ğŸ³ Docker Engine"
        D_CLI["ğŸ’» Docker CLI"] --> D_Daemon["ğŸ”§ dockerd<br/>Central daemon"]
        D_Daemon --> D_containerd["âš™ï¸ containerd"]
        D_containerd --> D_runc["ğŸƒ runc<br/>OCI runtime"]
    end
    
    subgraph "ğŸ¦­ Podman"
        P_CLI["ğŸ’» podman CLI"] --> P_Direct["âš¡ Direct to<br/>No daemon"]
        P_Direct --> P_runc["ğŸƒ runc/crun<br/>OCI runtime"]
    end
    
    subgraph "â˜¸ï¸ Kubernetes"
        K_kubelet["ğŸ¯ kubelet"] --> K_CRI["ğŸ”Œ CRI Plugin"]
        K_CRI --> K_containerd["âš™ï¸ containerd"]
        K_CRI --> K_CRIO["ğŸ¯ CRI-O"]
        K_containerd --> K_runc["ğŸƒ runc"]
        K_CRIO --> K_runc2["ğŸƒ runc/crun"]
    end
    
    style D_CLI fill:#e8f4f8,stroke:#2c3e50,color:#2c3e50
    style D_Daemon fill:#fdebd0,stroke:#2c3e50,color:#2c3e50
    style P_CLI fill:#e8f4f8,stroke:#2c3e50,color:#2c3e50
    style P_Direct fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
    style K_kubelet fill:#f4ecf7,stroke:#2c3e50,color:#2c3e50
```

* ğŸ” **Detailed Comparison**:

| **Feature** | **ğŸ³ Docker** | **ğŸ¦­ Podman** | **âš™ï¸ containerd** | **ğŸ¯ CRI-O** |
|-------------|--------------|---------------|------------------|--------------|
| **Architecture** | Client-server daemon | Daemonless | Core runtime | Kubernetes-focused |
| **Root Required** | âš ï¸ Yes (by default) | âœ… No (rootless mode) | âš ï¸ Depends | âœ… Rootless support |
| **Systemd Integration** | ğŸ”§ Via daemon | âœ… Native systemd | ğŸ”§ Via plugins | âœ… Native systemd |
| **Kubernetes Use** | ğŸ”„ Via containerd | âœ… Direct CRI support | âœ… Native K8s runtime | âœ… Built for K8s |
| **Build Support** | âœ… docker build | âœ… podman build | âŒ Needs buildkit | âŒ External builder |
| **Docker Compose** | âœ… Native | âœ… podman-compose | âŒ No | âŒ No |
| **Swarm Mode** | âœ… Built-in | âŒ No (use K8s) | âŒ No | âŒ No |
| **OCI Compliant** | âœ… Yes | âœ… Yes | âœ… Yes | âœ… Yes |
| **Registry** | ğŸ³ Docker Hub default | ğŸŒ Multiple defaults | ğŸŒ Configurable | ğŸŒ Configurable |

* ğŸ³ **Docker Engine**:
  * ğŸ¢ **Market Leader**: Most widely adopted, extensive documentation
  * ğŸ”§ **Components**: dockerd (daemon), containerd (runtime), runc (OCI)
  * ğŸ“¦ **Extras**: Docker Compose, Docker Desktop, Docker Hub integration
  * âš ï¸ **Considerations**: Requires root/daemon, single point of failure
  * ğŸ¯ **Best For**: Development, legacy systems, comprehensive tooling

* ğŸ¦­ **Podman (Pod Manager)**:
  * ğŸ”’ **Security First**: No daemon = smaller attack surface, rootless by default
  * ğŸ”„ **Docker Compatible**: Drop-in replacement (`alias docker=podman`)
  * ğŸ—ï¸ **Pod Support**: Native pod concept (group of containers), like Kubernetes
  * ğŸ§© **Systemd Integration**: Generate systemd units for containers
  * ğŸ¯ **Best For**: Rootless containers, systemd integration, security-focused

* âš™ï¸ **containerd**:
  * ğŸ§± **Core Runtime**: Extracted from Docker, donated to CNCF
  * â˜¸ï¸ **K8s Standard**: Default runtime for Kubernetes 1.24+
  * ğŸ”Œ **CRI (Container Runtime Interface) Compatible**: Direct K8s integration
  * ğŸ¯ **Minimal**: No build tools, focused on runtime operations
  * ğŸ¯ **Best For**: Kubernetes clusters, production workloads, minimal overhead

* ğŸ¯ **CRI-O**:
  * â˜¸ï¸ **Kubernetes Native**: Built specifically for K8s, implements CRI spec
  * ğŸª¶ **Lightweight**: Minimal dependencies, fast startup
  * ğŸ”’ **Security Focused**: SELinux/AppArmor support, user namespaces
  * ğŸ¯ **OCI First**: Direct OCI implementation without legacy code
  * ğŸ¯ **Best For**: Production Kubernetes, OpenShift, security-critical workloads

* ğŸ”„ **Evolution & Industry Trends**:
  * ğŸ“‰ **Docker Deprecation in K8s**: Kubernetes removed dockershim in v1.24 (2022)
  * ğŸ“ˆ **containerd Rise**: Now default for most managed Kubernetes services
  * ğŸ¦­ **Podman Growth**: Adopted by Red Hat, Fedora, security-conscious orgs
  * ğŸ¯ **CRI-O Adoption**: Standard in OpenShift, growing in enterprise K8s
  * ğŸŒ **OCI Standardization**: All engines converging on OCI specs

ğŸ”— **Resources:**
* [Docker Engine Overview](https://docs.docker.com/engine/)
* [Podman Documentation](https://docs.podman.io/)
* [containerd Website](https://containerd.io/)
* [CRI-O Documentation](https://cri-o.io/)
* [Don't Panic: Kubernetes and Docker](https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/)

---

## ğŸ“ Slide 7 â€“ ğŸ” OCI (Open Container Initiative) Standards

* ğŸ“œ **OCI = Open Container Initiative** - Linux Foundation project to create open industry standards for container formats and runtimes
* ğŸ¯ **Mission**: Ensure containers built with one tool can run with any OCI-compliant runtime
* ğŸ“¦ **Founded**: June 2015 by Docker, CoreOS, Google, Microsoft, Red Hat, and others
* ğŸ§© **Three Core Specifications**: Image-spec, Runtime-spec, Distribution-spec
* âœ… **Compliance**: Prevents vendor lock-in, enables interoperability across container ecosystems

**OCI Standards Architecture**
```mermaid
flowchart LR
    subgraph "ğŸ›ï¸ OCI Standards"
        Image["ğŸ“¦ Image Spec<br/>How images are built<br/>Layer format, manifest"]
        Runtime["ğŸƒ Runtime Spec<br/>How to run containers<br/>Config, lifecycle"]
        Dist["ğŸŒ Distribution Spec<br/>How to share images<br/>Registry API"]
    end
    
    subgraph "ğŸ› ï¸ Implementations"
        BuildTools["ğŸ”¨ Build Tools<br/>Docker, Buildah<br/>Kaniko, img"]
        Runtimes["âš™ï¸ Runtimes<br/>runc, crun<br/>Kata, gVisor"]
        Registries["ğŸ—„ï¸ Registries<br/>Docker Hub, Quay<br/>GHCR, Harbor"]
    end
    
    Image --> BuildTools
    Runtime --> Runtimes
    Dist --> Registries
    
    style Image fill:#e8f4f8,stroke:#2c3e50,color:#2c3e50
    style Runtime fill:#fef5e7,stroke:#2c3e50,color:#2c3e50
    style Dist fill:#f4ecf7,stroke:#2c3e50,color:#2c3e50
    style BuildTools fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
    style Runtimes fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
    style Registries fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
```

* ğŸ“¦ **Image Specification (image-spec)**:
  * ğŸ‚ **Layer Format**: How filesystem layers are stored and stacked
  * ğŸ“‹ **Manifest**: JSON document describing image contents
  * âš™ï¸ **Configuration**: Image config (ENV, CMD, exposed ports)
  * ğŸ·ï¸ **Content Addressing**: Images identified by SHA-256 digest
  * ğŸ“ **Layout**: Directory structure for OCI image on disk

* ğŸƒ **Runtime Specification (runtime-spec)**:
  * ğŸ—‚ï¸ **Filesystem Bundle**: Container root filesystem + config.json
  * âš™ï¸ **Configuration**: Runtime parameters (process, mounts, namespaces)
  * ğŸ”„ **Lifecycle**: Operations (create, start, kill, delete)
  * ğŸ§© **Hooks**: Prestart, poststart, poststop customization
  * ğŸ§ **Platform-Specific**: Linux, Windows, Solaris support

* ğŸŒ **Distribution Specification (distribution-spec)**:
  * ğŸš€ **Registry API**: HTTP API for pushing/pulling images
  * ğŸ“¦ **Manifest Format**: How to describe multi-platform images
  * ğŸ” **Authentication**: Token-based auth for registries
  * ğŸ·ï¸ **Tag Operations**: Listing, deletion, manifest retrieval
  * ğŸŒ **Content Discovery**: Finding and retrieving image layers

* âœ… **OCI Compliance Benefits**:
  * ğŸ”„ **Interoperability**: Build with Docker, run with Podman or CRI-O
  * ğŸš« **No Vendor Lock-in**: Switch runtimes without rebuilding images
  * ğŸ›¡ï¸ **Security**: Standardized security model and audit surface
  * ğŸ“ˆ **Innovation**: Multiple competing implementations improve ecosystem
  * ğŸ§ª **Testing**: Standard conformance tests ensure compatibility

**OCI Ecosystem Flow**
```mermaid
flowchart LR
    Dev["ğŸ‘¨â€ğŸ’» Developer"] --> Build["ğŸ”¨ Build Tool<br/>Docker/Buildah"]
    Build --> Image["ğŸ“¦ OCI Image<br/>Standard format"]
    Image --> Registry["ğŸ—„ï¸ OCI Registry<br/>Harbor/Quay"]
    Registry --> Pull["ğŸ“¥ Pull Image"]
    Pull --> Runtime["ğŸƒ OCI Runtime<br/>runc/crun"]
    Runtime --> Container["ğŸ“¦ Running Container<br/>Standard behavior"]
    
    style Dev fill:#e8f4f8,stroke:#2c3e50,color:#2c3e50
    style Build fill:#fef5e7,stroke:#2c3e50,color:#2c3e50
    style Image fill:#f4ecf7,stroke:#2c3e50,color:#2c3e50
    style Registry fill:#fdebd0,stroke:#2c3e50,color:#2c3e50
    style Pull fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
    style Runtime fill:#a9dfbf,stroke:#2c3e50,color:#2c3e50
    style Container fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
```

* ğŸ›ï¸ **Founding Members & Governance**:
  * ğŸ³ Docker (contributed original container format)
  * ğŸ”´ Red Hat / CoreOS (contributed rkt container format)
  * â˜ï¸ Google (Kubernetes integration)
  * ğŸªŸ Microsoft (Windows container support)
  * ğŸ AWS, Google Cloud, Azure (cloud provider backing)
  * ğŸ§± Governance: Technical Oversight Board + maintainers

ğŸ”— **Resources:**
* [OCI Official Website](https://opencontainers.org/)
* [OCI Image Specification](https://github.com/opencontainers/image-spec)
* [OCI Runtime Specification](https://github.com/opencontainers/runtime-spec)
* [OCI Distribution Specification](https://github.com/opencontainers/distribution-spec)

---

## ğŸ“ Slide 8 â€“ âš™ï¸ Container Runtime Internals (runc, crun, Kata Containers, gVisor)

* ğŸƒ **Container Runtime** = low-level software that creates and runs containers according to OCI runtime specification
* ğŸ§± **runc** = reference implementation of OCI runtime spec, written in Go, used by Docker/containerd/CRI-O
* âš¡ **crun** = Fast, lightweight OCI runtime written in C, used by Podman and CRI-O
* ğŸ›¡ï¸ **Kata Containers** = Secure runtime using lightweight VMs (Virtual Machines) for stronger isolation
* ğŸ”’ **gVisor** = Google's sandboxed runtime using user-space kernel (defense in depth)

**Runtime Stack Layers**
```mermaid
flowchart LR
    subgraph "High-Level Tools"
        Docker["ğŸ³ Docker CLI"]
        Podman["ğŸ¦­ Podman CLI"]
        Kubectl["â˜¸ï¸ kubectl"]
    end
    
    subgraph "Container Engines"
        dockerd["ğŸ”§ dockerd"]
        containerd["âš™ï¸ containerd"]
        CRIO["ğŸ¯ CRI-O"]
    end
    
    subgraph "OCI Runtimes"
        runc["ğŸƒ runc<br/>Standard"]
        crun["âš¡ crun<br/>Fast C impl"]
        kata["ğŸ›¡ï¸ Kata<br/>VM-based"]
        gvisor["ğŸ”’ gVisor<br/>Sandboxed"]
    end
    
    subgraph "Kernel"
        kernel["ğŸ§ Linux Kernel<br/>Namespaces, cgroups"]
    end
    
    Docker --> dockerd
    Podman --> containerd
    Kubectl --> CRIO
    
    dockerd --> containerd
    containerd --> runc
    containerd --> kata
    CRIO --> runc
    CRIO --> crun
    CRIO --> gvisor
    
    runc --> kernel
    crun --> kernel
    kata --> kernel
    gvisor --> kernel
    
    style Docker fill:#e8f4f8,stroke:#2c3e50,color:#2c3e50
    style runc fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
    style crun fill:#fef5e7,stroke:#2c3e50,color:#2c3e50
    style kata fill:#f4ecf7,stroke:#2c3e50,color:#2c3e50
    style gvisor fill:#fdebd0,stroke:#2c3e50,color:#2c3e50
```

* ğŸƒ **runc - The Standard Runtime**:
  * ğŸ“¦ **Origin**: Extracted from Docker libcontainer, donated to OCI
  * ğŸ§¬ **Written in**: Go language
  * ğŸ¯ **Features**: Full OCI spec implementation, battle-tested
  * âš¡ **Performance**: Good balance of speed and features
  * ğŸ”§ **Usage**: Default for Docker, containerd, many Kubernetes clusters
  * ğŸ“Š **Market Share**: ~80% of container runtimes

* âš¡ **crun - The Fast Runtime**:
  * âš™ï¸ **Written in**: C language (vs Go for runc)
  * ğŸš€ **Speed**: 2x faster startup, 30% less memory than runc
  * ğŸ”’ **Security**: Smaller codebase = smaller attack surface
  * ğŸ§© **Features**: Full OCI compliance, cgroups v2 support
  * ğŸ¯ **Adoption**: Default in Podman, optional in CRI-O
  * ğŸ“Š **Benchmarks**: 50ms startup vs 100ms for runc

* ğŸ›¡ï¸ **Kata Containers - VM-Isolated Runtime**:
  * ğŸ–¥ï¸ **Architecture**: Each container runs in lightweight VM (Qemu/Firecracker)
  * ğŸ”’ **Isolation**: Hardware virtualization = stronger security boundary
  * âš–ï¸ **Trade-off**: More overhead than runc (~125MB memory), but VM-level isolation
  * ğŸ¯ **Use Cases**: Multi-tenant environments, untrusted code, compliance
  * â˜¸ï¸ **K8s Integration**: RuntimeClass for mixed workloads

**Kata Containers Architecture**
```mermaid
flowchart LR
    subgraph "Host"
        containerd["âš™ï¸ containerd"]
        kata_shim["ğŸ›¡ï¸ Kata Shim"]
    end
    
    subgraph "Lightweight VM"
        guest_kernel["ğŸ§ Guest Kernel"]
        kata_agent["ğŸ“¦ Kata Agent"]
        container_app["ğŸ“± Container App<br/>Isolated in VM"]
    end
    
    containerd --> kata_shim
    kata_shim --> guest_kernel
    guest_kernel --> kata_agent
    kata_agent --> container_app
    
    style containerd fill:#e8f4f8,stroke:#2c3e50,color:#2c3e50
    style kata_shim fill:#f4ecf7,stroke:#2c3e50,color:#2c3e50
    style guest_kernel fill:#fef5e7,stroke:#2c3e50,color:#2c3e50
    style container_app fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
```

* ğŸ”’ **gVisor - Sandboxed Runtime**:
  * ğŸ§± **Architecture**: User-space kernel (Sentry) intercepts syscalls
  * ğŸ›¡ï¸ **Isolation**: Application doesn't directly access host kernel
  * ğŸ“‰ **Trade-off**: 10-15% performance overhead vs runc
  * ğŸ¯ **Use Cases**: Serverless platforms (Google Cloud Run), multi-tenant SaaS
  * ğŸ§ª **Compatibility**: ~70% syscall coverage (not all apps work)

**gVisor Architecture**
```mermaid
flowchart LR
    subgraph "Container Process"
        app["ğŸ“± Application<br/>Makes syscalls"]
    end
    
    subgraph "gVisor Sandbox"
        sentry["ğŸ›¡ï¸ Sentry<br/>User-space kernel<br/>Syscall handler"]
        gofer["ğŸ“ Gofer<br/>File proxy"]
    end
    
    subgraph "Host"
        host_kernel["ğŸ§ Host Kernel<br/>Limited access"]
    end
    
    app --> sentry
    sentry --> gofer
    gofer --> host_kernel
    sentry --> host_kernel
    
    style app fill:#e8f4f8,stroke:#2c3e50,color:#2c3e50
    style sentry fill:#fdebd0,stroke:#2c3e50,color:#2c3e50
    style gofer fill:#f4ecf7,stroke:#2c3e50,color:#2c3e50
    style host_kernel fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
```

* ğŸ“Š **Runtime Comparison Table**:

| **Runtime** | **ğŸƒ runc** | **âš¡ crun** | **ğŸ›¡ï¸ Kata** | **ğŸ”’ gVisor** |
|-------------|------------|-----------|------------|-------------|
| **Language** | Go | C | Go/Rust | Go |
| **Startup** | 100ms | 50ms | 500ms | 150ms |
| **Memory** | 30MB | 20MB | 150MB | 40MB |
| **Performance** | Baseline | 102% | 70% | 85% |
| **Isolation** | Namespace | Namespace | VM | Syscall filter |
| **Security** | Good | Good | Excellent | Excellent |
| **Compatibility** | 100% | 100% | 95% | 70% |
| **Use Case** | General | Performance | Security | Sandboxing |

ğŸ”— **Resources:**
* [runc GitHub](https://github.com/opencontainers/runc)
* [crun GitHub](https://github.com/containers/crun)
* [Kata Containers](https://katacontainers.io/)
* [gVisor Documentation](https://gvisor.dev/)

---

## ğŸ“ Slide 9 â€“ ğŸ—ï¸ Linux Kernel Features (Namespaces, cgroups, Capabilities, SELinux/AppArmor)

* ğŸ§ **Linux Kernel = Container Foundation** - Containers are NOT a kernel feature; they're built from combining multiple kernel primitives
* ğŸ§© **Namespaces** = Isolation of system resources (what a process can see)
* ğŸ“Š **cgroups (Control Groups)** = Resource limiting and accounting (what a process can use)
* ğŸ”‘ **Capabilities** = Fine-grained privilege control (what a process can do)
* ğŸ›¡ï¸ **Security Modules** = SELinux/AppArmor for Mandatory Access Control (MAC = Mandatory Access Control)

**Kernel Features Building Blocks**
```mermaid
flowchart LR
    subgraph "ğŸ§ Linux Kernel Features"
        NS["ğŸ§© Namespaces<br/>Isolation<br/>What you see"]
        CG["ğŸ“Š cgroups<br/>Limits<br/>What you use"]
        CAP["ğŸ”‘ Capabilities<br/>Privileges<br/>What you can do"]
        SEC["ğŸ›¡ï¸ Security Modules<br/>MAC<br/>Policy enforcement"]
    end
    
    subgraph "ğŸ³ Container Runtime"
        runtime["âš™ï¸ runc/crun<br/>Combines features"]
    end
    
    subgraph "ğŸ“¦ Running Container"
        container["ğŸ“¦ Isolated Process<br/>Limited resources<br/>Restricted permissions<br/>Policy-enforced"]
    end
    
    NS --> runtime
    CG --> runtime
    CAP --> runtime
    SEC --> runtime
    runtime --> container
    
    style NS fill:#e8f4f8,stroke:#2c3e50,color:#2c3e50
    style CG fill:#fef5e7,stroke:#2c3e50,color:#2c3e50
    style CAP fill:#f4ecf7,stroke:#2c3e50,color:#2c3e50
    style SEC fill:#fdebd0,stroke:#2c3e50,color:#2c3e50
    style runtime fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
    style container fill:#a9dfbf,stroke:#2c3e50,color:#2c3e50
```

* ğŸ§© **Namespaces - Process Isolation (7 Types)**:
  * ğŸ†” **PID (Process ID)**: Isolated process tree, PID 1 inside container
  * ğŸŒ **Network**: Separate network stack (interfaces, routes, firewall rules)
  * ğŸ”ï¸ **Mount**: Isolated filesystem mount points
  * ğŸ”¤ **UTS (Unix Timesharing System)**: Hostname and domain name isolation
  * ğŸ“¬ **IPC (Inter-Process Communication)**: Separate message queues, semaphores
  * ğŸ‘¤ **User**: Map container root to non-root host user (rootless containers)
  * â° **Time**: Isolated system clock (Linux 5.6+, for time-sensitive apps)

* ğŸ“Š **cgroups - Resource Control (Control Groups)**:
  * ğŸ§  **CPU**: Limit CPU shares, cores, throttling
  * ğŸ’¾ **Memory**: Set memory limits, swap, OOM (Out Of Memory) behavior
  * ğŸ’¿ **I/O**: Disk read/write limits, IOPS (Input/Output Operations Per Second)
  * ğŸŒ **Network**: Bandwidth limiting (via tc = traffic control)
  * ğŸ¯ **Accounting**: Track resource usage per container
  * ğŸ”„ **Versions**: cgroups v1 (legacy) vs v2 (unified hierarchy, modern)

**cgroups Hierarchy**
```mermaid
flowchart LR
    subgraph "ğŸ“Š cgroups v2 Hierarchy"
        root["/sys/fs/cgroup/"]
        docker_slice["docker.slice/"]
        container1["docker-abc123.scope"]
        container2["docker-def456.scope"]
        
        subgraph "Per-Container Limits"
            cpu["ğŸ§  cpu.max<br/>CPU quota"]
            mem["ğŸ’¾ memory.max<br/>RAM limit"]
            io["ğŸ’¿ io.max<br/>Disk limit"]
        end
    end
    
    root --> docker_slice
    docker_slice --> container1
    docker_slice --> container2
    container1 --> cpu
    container1 --> mem
    container1 --> io
    
    style root fill:#e8f4f8,stroke:#2c3e50,color:#2c3e50
    style docker_slice fill:#fef5e7,stroke:#2c3e50,color:#2c3e50
    style container1 fill:#f4ecf7,stroke:#2c3e50,color:#2c3e50
```

* ğŸ”‘ **Capabilities - Fine-Grained Privileges**:
  * ğŸ›¡ï¸ **Concept**: Split root privileges into discrete units
  * ğŸ“‹ **Examples**: CAP_NET_BIND_SERVICE (bind port <1024), CAP_SYS_ADMIN (mount), CAP_CHOWN (change file owner)
  * ğŸš« **Default Drop**: Containers drop dangerous capabilities by default
  * âœ… **Add Selectively**: Only grant needed capabilities (principle of least privilege)
  * ğŸ”’ **Rootless**: Run containers as non-root user with limited capabilities

```bash
# ğŸ” View default container capabilities
docker run --rm alpine sh -c 'cat /proc/self/status | grep Cap'

# ğŸš« Drop all capabilities, add only specific ones
docker run --rm \
  --cap-drop=ALL \
  --cap-add=NET_BIND_SERVICE \
  nginx

# ğŸ§ª Try to use dropped capability (will fail)
docker run --rm \
  --cap-drop=CHOWN \
  alpine chown nobody:nobody /tmp  # âŒ Operation not permitted
```

* ğŸ›¡ï¸ **SELinux/AppArmor - Mandatory Access Control**:
  * ğŸ” **Purpose**: Enforce security policies beyond traditional permissions
  * ğŸ¢ **SELinux (Security-Enhanced Linux)**: Label-based, used by RHEL/CentOS/Fedora
  * ğŸ **AppArmor**: Path-based, used by Ubuntu/Debian/SUSE
  * ğŸ§© **Container Policies**: Restrict syscalls, file access, network operations
  * âš ï¸ **Modes**: Enforcing (block violations), Permissive (log only), Disabled

* ğŸ§  **How They Work Together**:
  * ğŸ§© **Namespaces**: Container sees separate system
  * ğŸ“Š **cgroups**: Container limited to allocated resources
  * ğŸ”‘ **Capabilities**: Container has restricted permissions
  * ğŸ›¡ï¸ **MAC (Mandatory Access Control)**: Kernel enforces security policy

**Complete Isolation Stack**
```mermaid
flowchart LR
    App["ğŸ“± App Process"] --> Namespaces["ğŸ§© Namespaces<br/>Isolated view"]
    Namespaces --> Capabilities["ğŸ”‘ Capabilities<br/>Limited privileges"]
    Capabilities --> cgroups["ğŸ“Š cgroups<br/>Resource limits"]
    cgroups --> SELinux["ğŸ›¡ï¸ SELinux/AppArmor<br/>Policy enforcement"]
    SELinux --> Kernel["ğŸ§ Linux Kernel<br/>System calls"]
    
    style App fill:#e8f4f8,stroke:#2c3e50,color:#2c3e50
    style Namespaces fill:#fef5e7,stroke:#2c3e50,color:#2c3e50
    style Capabilities fill:#f4ecf7,stroke:#2c3e50,color:#2c3e50
    style cgroups fill:#fdebd0,stroke:#2c3e50,color:#2c3e50
    style SELinux fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
    style Kernel fill:#a9dfbf,stroke:#2c3e50,color:#2c3e50
```

* ğŸ’¡ **Fun Fact**: Containers are just processes with fancy kernel features - you can create a "container" manually using `unshare`, `cgroups`, and `chroot`!

ğŸ”— **Resources:**
* [Linux Namespaces Documentation](https://man7.org/linux/man-pages/man7/namespaces.7.html)
* [cgroups v2 Documentation](https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html)
* [Linux Capabilities](https://man7.org/linux/man-pages/man7/capabilities.7.html)
* [Container Security Best Practices](https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html)

---
## ğŸ“ Slide 10 â€“ ğŸ‹ Docker Architecture (Client-Server, Daemon, CLI, Engine)

* ğŸ—ï¸ **Docker Architecture** = Client-server model with Docker daemon (dockerd) managing containers
* ğŸ’» **Docker Client** = CLI tool (`docker` command) that talks to daemon via REST API
* ğŸ”§ **Docker Daemon (dockerd)** = Background service managing images, containers, networks, volumes
* âš™ï¸ **Docker Engine** = Complete package: daemon + containerd + runc
* ğŸŒ **Remote Access** = Can control remote Docker hosts via TCP/SSH/socket

**Docker Architecture Components**
```mermaid
flowchart LR
    CLI["ğŸ’» Docker CLI<br/>docker run/build/ps"] --> API["ğŸ”Œ REST API<br/>Unix socket or TCP"]
    API --> Daemon["ğŸ”§ dockerd<br/>Docker Daemon"]
    Daemon --> containerd["âš™ï¸ containerd<br/>Container lifecycle"]
    containerd --> shim["ğŸ“¦ containerd-shim"]
    shim --> runc["ğŸƒ runc<br/>OCI runtime"]
    
    Daemon --> Images["ğŸ“¸ Image Management"]
    Daemon --> Networks["ğŸŒ Network Management"]
    Daemon --> Volumes["ğŸ’¾ Volume Management"]
    
    style CLI fill:#e8f4f8,stroke:#2c3e50,color:#2c3e50
    style Daemon fill:#fef5e7,stroke:#2c3e50,color:#2c3e50
    style containerd fill:#f4ecf7,stroke:#2c3e50,color:#2c3e50
    style runc fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
```

* ğŸ”‘ **Key Components**:
  * ğŸ–¥ï¸ **Docker Desktop**: GUI app for Mac/Windows (includes Docker Engine)
  * ğŸ³ **Docker Hub**: Default public registry for images
  * ğŸ”Œ **Docker API**: RESTful interface for automation
  * ğŸ§© **Docker Compose**: Multi-container orchestration tool

```bash
# ğŸ” Docker architecture inspection
docker version              # Shows client and server versions
docker info                 # Detailed system information
docker system df            # Disk usage by images/containers/volumes

# ğŸŒ Connect to remote Docker host
docker -H ssh://user@remote.host ps
docker -H tcp://192.168.1.10:2375 ps
```

ğŸ”— **Resources:**
* [Docker Architecture Overview](https://docs.docker.com/get-started/overview/)
* [Docker Engine API](https://docs.docker.com/engine/api/)

---

## ğŸ“ Slide 11 â€“ ğŸ“¦ Docker Images & Layers (Build Process, Caching, Multi-stage)

* ğŸ‚ **Image Layers** = Read-only filesystem layers stacked using Union FS (UnionFS = Union File System)
* ğŸ”„ **Layer Caching** = Unchanged layers reused in builds (speeds up rebuilds)
* ğŸ—ï¸ **Build Context** = Files sent to Docker daemon during build
* ğŸ¯ **Multi-stage Builds** = Use multiple FROM statements to reduce final image size
* ğŸ“ **Best Practice** = Order Dockerfile commands from least to most frequently changing

**Image Layer Stack**
```mermaid
flowchart LR
    Base["ğŸ§± FROM alpine:3.18<br/>Layer 1: 5MB"] --> Run1["âš™ï¸ RUN apk add python3<br/>Layer 2: +50MB"]
    Run1 --> Copy["ğŸ“ COPY requirements.txt<br/>Layer 3: +1KB"]
    Copy --> Run2["ğŸ“¦ RUN pip install<br/>Layer 4: +100MB"]
    Run2 --> Copy2["ğŸ“± COPY app.py<br/>Layer 5: +10KB"]
    Copy2 --> Final["ğŸ“¦ Final Image<br/>Total: 155MB"]
    
    style Base fill:#e8f4f8,stroke:#2c3e50,color:#2c3e50
    style Run1 fill:#fef5e7,stroke:#2c3e50,color:#2c3e50
    style Copy fill:#f4ecf7,stroke:#2c3e50,color:#2c3e50
    style Run2 fill:#fdebd0,stroke:#2c3e50,color:#2c3e50
    style Copy2 fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
    style Final fill:#a9dfbf,stroke:#2c3e50,color:#2c3e50
```

ğŸ”— **Resources:**
* [Dockerfile Best Practices](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/)
* [Multi-stage Builds](https://docs.docker.com/build/building/multi-stage/)

---

## ğŸ“ Slide 12 â€“ ğŸ¯ Dockerfile Best Practices (Optimization, Security, Size)

* ğŸ“ **Order Matters** = Place frequently changing commands last (maximize cache hits)
* ğŸª¶ **Minimize Layers** = Combine RUN commands with `&&` to reduce layers
* ğŸ§¹ **Clean Up** = Remove temporary files in same RUN command
* ğŸ”’ **Security** = Use non-root user, scan for vulnerabilities, minimal base images
* ğŸ“¦ **Size Optimization** = Use Alpine Linux, multi-stage builds, .dockerignore
* ğŸ”‘ **Key Optimizations**:
  * ğŸ“ **.dockerignore**: Exclude unnecessary files from build context
  * ğŸ—ï¸ **Layer Ordering**: Dependencies before code (better caching)
  * ğŸ§¹ **Cleanup in Same Layer**: `RUN apt-get update && apt-get install && rm -rf /var/lib/apt/lists/*`
  * ğŸ”’ **Non-root User**: Create and switch to unprivileged user
  * ğŸ¯ **Minimal Base**: Alpine (5MB) vs Ubuntu (77MB)

ğŸ”— **Resources:**
* [Docker Security Best Practices](https://snyk.io/blog/10-docker-image-security-best-practices/)
* [Dockerfile Optimization](https://docs.docker.com/develop/dev-best-practices/)

---

## ğŸ“ Slide 13 â€“ ğŸŒ Docker Networking (Bridge, Host, Overlay, Container Communication)

* ğŸŒ‰ **Bridge Network** = Default, isolated network for containers on single host
* ğŸ  **Host Network** = Container uses host's network stack directly (no isolation)
* ğŸŒ **Overlay Network** = Multi-host networking for Docker Swarm/services
* ğŸ”— **Container Links** = Deprecated; use user-defined networks instead
* ğŸ”Œ **Port Mapping** = `-p host:container` exposes container ports to host

**Docker Network Types**
```mermaid
flowchart LR
    subgraph "ğŸŒ‰ Bridge Network (Default)"
        C1["ğŸ“¦ Container 1<br/>172.17.0.2"]
        C2["ğŸ“¦ Container 2<br/>172.17.0.3"]
        Bridge["ğŸŒ‰ docker0 bridge<br/>172.17.0.1"]
    end
    
    subgraph "ğŸ  Host Network"
        C3["ğŸ“¦ Container 3<br/>Uses host IPs"]
    end
    
    subgraph "ğŸŒ Overlay Network"
        Node1["ğŸ–¥ï¸ Host 1"] --> OV["ğŸŒ Overlay"]
        Node2["ğŸ–¥ï¸ Host 2"] --> OV
    end
    
    C1 --> Bridge
    C2 --> Bridge
    Bridge --> Host["ğŸ’» Host Network"]
    C3 --> Host
    
    style Bridge fill:#e8f4f8,stroke:#2c3e50,color:#2c3e50
    style C3 fill:#fef5e7,stroke:#2c3e50,color:#2c3e50
    style OV fill:#f4ecf7,stroke:#2c3e50,color:#2c3e50
```

* ğŸ“Š **Network Drivers**:
  * ğŸŒ‰ **bridge**: Default, single host
  * ğŸ  **host**: No isolation, best performance
  * ğŸ”Œ **none**: No networking
  * ğŸŒ **overlay**: Multi-host (Swarm)
  * ğŸ”— **macvlan**: Assign MAC address to container

ğŸ”— **Resources:**
* [Docker Networking Overview](https://docs.docker.com/network/)
* [Container Networking Deep Dive](https://www.youtube.com/watch?v=bKFMS5C4CG0)

---

## ğŸ“ Slide 14 â€“ ğŸ’¾ Docker Storage (Volumes, Bind Mounts, tmpfs, Drivers)

* ğŸ’¾ **Volumes** = Managed by Docker, best for persistent data
* ğŸ“ **Bind Mounts** = Mount host directory into container
* âš¡ **tmpfs** = In-memory storage, fast but ephemeral
* ğŸ—„ï¸ **Storage Drivers** = overlay2 (default), btrfs, zfs, devicemapper
* ğŸ”„ **Container Layer** = Writable layer on top of read-only image layers

**Storage Options Comparison**
```mermaid
flowchart LR
    subgraph "ğŸ’¾ Docker Volume"
        Vol["ğŸ—„ï¸ Managed by Docker<br/>/var/lib/docker/volumes/<br/>Best practice"]
    end
    
    subgraph "ğŸ“ Bind Mount"
        Bind["ğŸ“‚ Host directory<br/>/home/user/data<br/>Development use"]
    end
    
    subgraph "âš¡ tmpfs"
        Tmp["ğŸ’¨ RAM-based<br/>Temporary data<br/>Fast, ephemeral"]
    end
    
    Container["ğŸ“¦ Container"] --> Vol
    Container --> Bind
    Container --> Tmp
    
    style Vol fill:#e8f4f8,stroke:#2c3e50,color:#2c3e50
    style Bind fill:#fef5e7,stroke:#2c3e50,color:#2c3e50
    style Tmp fill:#f4ecf7,stroke:#2c3e50,color:#2c3e50
```

* ğŸ¯ **When to Use**:
  * ğŸ’¾ **Volumes**: Databases, application data, production
  * ğŸ“ **Bind Mounts**: Development, config files, logs
  * âš¡ **tmpfs**: Sensitive data, temporary processing

ğŸ”— **Resources:**
* [Docker Storage Overview](https://docs.docker.com/storage/)
* [Volume vs Bind Mount](https://docs.docker.com/storage/volumes/)

---

### ğŸ­ **Interactive Break #1: "Container Horror Stories"**

**ğŸ‘» Share Your Container Nightmares!**

*Time for some fun! Let's hear about container disasters - learning from mistakes is the best education!*

**ğŸ”¥ Common Horror Stories:**

1. **ğŸ’£ "The 300GB Log File"**
   * Developer ran container without volume
   * App logged to container filesystem
   * Filled up entire host disk
   * All containers crashed
   * **Lesson**: Always use volumes for logs! ğŸ“

2. **ğŸ³ "The Zombie Container Army"**
   * CI/CD pipeline didn't clean up
   * `docker ps -a` showed 10,000+ stopped containers
   * Host ran out of disk space
   * **Lesson**: Use `--rm` flag or cleanup scripts! ğŸ§¹

3. **ğŸ”’ "The Exposed Database"**
   * Accidentally used `-p 5432:5432` on public server
   * PostgreSQL open to internet
   * Got hacked in 3 hours
   * **Lesson**: Never expose databases directly! ğŸ›¡ï¸

4. **ğŸ·ï¸ "The Latest Tag Disaster"**
   * Production used `image:latest`
   * Latest changed overnight
   * Production broke at 3 AM
   * **Lesson**: Pin specific versions! ğŸ“Œ

5. **ğŸ’¾ "The Lost Data Tragedy"**
   * Database container without volume
   * `docker-compose down`
   * All customer data gone
   * **Lesson**: Volumes for stateful apps! ğŸ’¾

**ğŸ¤” Discussion Questions:**
1. What's the weirdest container issue you've encountered?
2. Ever accidentally deleted production data?
3. What's your container backup strategy?
4. How do you prevent container sprawl?

**ğŸ¯ Quick Quiz:**
* Q: What happens to data when you delete a container without volumes?
* A: ğŸ’€ It's gone forever!

*Remember: Every expert was once a beginner who didn't give up after their first container disaster!* ğŸ˜„

---

## ğŸ“ Slide 15 â€“ â˜¸ï¸ Introduction to Kubernetes (What, Why, Architecture)

* â˜¸ï¸ **Kubernetes (K8s)** = Open-source container orchestration platform for automating deployment, scaling, and management
* ğŸ¯ **Born at Google** = Based on 15+ years of running containers (Borg/Omega systems)
* ğŸŒ **CNCF Project** = Donated to Cloud Native Computing Foundation in 2015
* ğŸš€ **Problem It Solves** = Managing hundreds/thousands of containers across multiple hosts
* ğŸ“Š **Market Dominance** = De facto standard for container orchestration (78% adoption)

**Kubernetes solves Container Orchestration Challenges**
```mermaid
flowchart LR
    subgraph "âŒ Without Kubernetes"
        P1["ğŸ¤¯ Manual scaling<br/>SSH to each host"]
        P2["ğŸ’” No self-healing<br/>Manual restart"]
        P3["ğŸŒ Complex networking<br/>Manual load balancing"]
        P4["ğŸ“¦ Hard deployments<br/>Downtime required"]
    end
    
    subgraph "âœ… With Kubernetes"
        S1["âš¡ Auto-scaling<br/>Based on metrics"]
        S2["ğŸ”„ Self-healing<br/>Auto restart/replace"]
        S3["ğŸŒ‰ Service discovery<br/>Built-in load balancing"]
        S4["ğŸš€ Rolling updates<br/>Zero-downtime deploys"]
    end
    
    P1 --> S1
    P2 --> S2
    P3 --> S3
    P4 --> S4
    
    style P1 fill:#fadbd8,stroke:#2c3e50,color:#2c3e50
    style S1 fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
```

* ğŸ¯ **Core Capabilities**:
  * ğŸ“¦ **Service Discovery & Load Balancing**: Automatic DNS and traffic distribution
  * ğŸ’¾ **Storage Orchestration**: Auto-mount cloud/local storage
  * ğŸ”„ **Automated Rollouts/Rollbacks**: Gradual deployment with health checks
  * ğŸ“Š **Automatic Bin Packing**: Optimal container placement based on resources
  * ğŸ”’ **Secret & Config Management**: Secure handling of sensitive data
  * ğŸ”§ **Self-Healing**: Restart failed containers, replace/reschedule

ğŸ”— **Resources:**
* [Kubernetes Official Docs](https://kubernetes.io/docs/home/)
* [Kubernetes the Hard Way](https://github.com/kelseyhightower/kubernetes-the-hard-way)
* [CNCF Kubernetes Overview](https://www.cncf.io/projects/kubernetes/)

---

## ğŸ“ Slide 16 â€“ ğŸ§© Kubernetes Core Components (Control Plane, Nodes, etcd)

* ğŸ¯ **Control Plane** = Manages the cluster, makes decisions, detects/responds to events
* ğŸ–¥ï¸ **Worker Nodes** = Run containerized applications (Pods)
* ğŸ—„ï¸ **etcd** = Distributed key-value store for all cluster data
* ğŸ”„ **Declarative Model** = Describe desired state, K8s makes it happen
* ğŸŒ **Highly Available** = Control plane can run on multiple nodes

**Kubernetes Cluster Architecture**
```mermaid
flowchart LR
    subgraph "ğŸ¯ Control Plane (Master)"
        API["ğŸŒ API Server<br/>kubectl endpoint"]
        Sched["ğŸ“… Scheduler<br/>Assigns pods to nodes"]
        CM["ğŸ›ï¸ Controller Manager<br/>Reconciliation loops"]
        etcd["ğŸ—„ï¸ etcd<br/>Cluster state DB"]
    end
    
    subgraph "ğŸ–¥ï¸ Worker Node 1"
        kubelet1["ğŸ¤– kubelet<br/>Node agent"]
        proxy1["ğŸŒ‰ kube-proxy<br/>Network rules"]
        runtime1["ğŸ³ Container Runtime<br/>containerd"]
        pod1["ğŸ“¦ Pods"]
    end
    
    subgraph "ğŸ–¥ï¸ Worker Node 2"
        kubelet2["ğŸ¤– kubelet"]
        proxy2["ğŸŒ‰ kube-proxy"]
        runtime2["ğŸ³ Container Runtime"]
        pod2["ğŸ“¦ Pods"]
    end
    
    API --> etcd
    API --> Sched
    API --> CM
    
    kubelet1 --> API
    kubelet2 --> API
    
    kubelet1 --> runtime1
    runtime1 --> pod1
    
    kubelet2 --> runtime2
    runtime2 --> pod2
    
    style API fill:#e8f4f8,stroke:#2c3e50,color:#2c3e50
    style etcd fill:#fef5e7,stroke:#2c3e50,color:#2c3e50
    style kubelet1 fill:#f4ecf7,stroke:#2c3e50,color:#2c3e50
    style pod1 fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
```

* ğŸ¯ **Control Plane Components**:
  * ğŸŒ **kube-apiserver**: Front-end REST API, only component that talks to etcd
  * ğŸ“… **kube-scheduler**: Watches for new pods, assigns to nodes based on resources
  * ğŸ›ï¸ **kube-controller-manager**: Runs controller loops (node, replication, endpoints, service accounts)
  * â˜ï¸ **cloud-controller-manager**: Integrates with cloud providers (optional)

* ğŸ–¥ï¸ **Worker Node Components**:
  * ğŸ¤– **kubelet**: Agent that ensures containers are running in pods
  * ğŸŒ‰ **kube-proxy**: Network proxy, maintains network rules, enables service communication
  * ğŸ³ **Container Runtime**: Runs containers (containerd, CRI-O, Docker deprecated)

ğŸ”— **Resources:**
* [Kubernetes Components](https://kubernetes.io/docs/concepts/overview/components/)
* [Understanding etcd](https://etcd.io/docs/)

---

## ğŸ“ Slide 17 â€“ ğŸ¯ Kubernetes Objects (Pods, Deployments, Services, ConfigMaps, Secrets)

* ğŸ“¦ **Pod** = Smallest deployable unit, one or more containers sharing network/storage
* ğŸš€ **Deployment** = Manages replica pods, rolling updates, rollbacks
* ğŸŒ **Service** = Stable network endpoint for accessing pods (load balancing)
* âš™ï¸ **ConfigMap** = Configuration data (non-sensitive)
* ğŸ”’ **Secret** = Sensitive data (passwords, tokens, keys)

**Kubernetes Object Hierarchy**
```mermaid
flowchart LR
    User["ğŸ‘¨â€ğŸ’» User"] --> Deployment["ğŸš€ Deployment<br/>Desired replicas: 3"]
    Deployment --> RS["ğŸ“‹ ReplicaSet<br/>Manages pods"]
    RS --> Pod1["ğŸ“¦ Pod 1<br/>Container(s)"]
    RS --> Pod2["ğŸ“¦ Pod 2<br/>Container(s)"]
    RS --> Pod3["ğŸ“¦ Pod 3<br/>Container(s)"]
    
    Service["ğŸŒ Service<br/>Load balancer"] --> Pod1
    Service --> Pod2
    Service --> Pod3
    
    Pod1 --> CM["âš™ï¸ ConfigMap<br/>Config data"]
    Pod1 --> Secret["ğŸ”’ Secret<br/>Passwords"]
    
    style Deployment fill:#e8f4f8,stroke:#2c3e50,color:#2c3e50
    style Service fill:#fef5e7,stroke:#2c3e50,color:#2c3e50
    style Pod1 fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
```

ğŸ”— **Resources:**
* [Kubernetes Objects Overview](https://kubernetes.io/docs/concepts/overview/working-with-objects/)
* [Pod Lifecycle](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/)

---

## ğŸ“ Slide 18 â€“ ğŸŒ Kubernetes Networking (CNI, Services, Ingress, NetworkPolicies)

* ğŸ”Œ **CNI (Container Network Interface)** = Plugin architecture for pod networking
* ğŸŒ‰ **Service Types** = ClusterIP (internal), NodePort (external), LoadBalancer (cloud)
* ğŸŒ **Ingress** = HTTP/HTTPS routing to services (reverse proxy)
* ğŸ›¡ï¸ **NetworkPolicy** = Firewall rules for pod communication
* ğŸ“¡ **DNS** = Automatic service discovery via CoreDNS

**Kubernetes Networking Model**
```mermaid
flowchart LR
    Internet["ğŸŒ Internet"] --> Ingress["ğŸŒ Ingress<br/>nginx/traefik<br/>HTTP routing"]
    Ingress --> Service1["ğŸŒ‰ Service<br/>web-service<br/>ClusterIP"]
    Ingress --> Service2["ğŸŒ‰ Service<br/>api-service<br/>ClusterIP"]
    
    Service1 --> Pod1["ğŸ“¦ Pod 1<br/>10.244.1.5"]
    Service1 --> Pod2["ğŸ“¦ Pod 2<br/>10.244.2.7"]
    Service2 --> Pod3["ğŸ“¦ Pod 3<br/>10.244.1.8"]
    
    NP["ğŸ›¡ï¸ NetworkPolicy<br/>Allow/Deny rules"] --> Pod1
    
    style Ingress fill:#e8f4f8,stroke:#2c3e50,color:#2c3e50
    style Service1 fill:#fef5e7,stroke:#2c3e50,color:#2c3e50
    style Pod1 fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
```

* ğŸ”Œ **Popular CNI Plugins**:
  * ğŸŒ **Calico**: Network policies, BGP routing
  * ğŸŒŠ **Cilium**: eBPF-based, high performance
  * ğŸ”— **Flannel**: Simple, overlay network
  * ğŸŒ‰ **Weave Net**: Mesh networking

ğŸ”— **Resources:**
* [Kubernetes Networking Model](https://kubernetes.io/docs/concepts/services-networking/)
* [Ingress Controllers](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/)

---

## ğŸ“ Slide 19 â€“ ğŸ’¾ Kubernetes Storage (PersistentVolumes, PVCs, StorageClasses)

* ğŸ’¾ **PersistentVolume (PV)** = Cluster storage resource (admin-provisioned or dynamic)
* ğŸ“‹ **PersistentVolumeClaim (PVC)** = Request for storage by a pod
* ğŸ—ï¸ **StorageClass** = Dynamic provisioning profile (cloud disks, NFS, etc.)
* ğŸ”„ **Volume Lifecycle** = Provisioning â†’ Binding â†’ Using â†’ Reclaiming
* ğŸ“¦ **Access Modes** = ReadWriteOnce (RWO), ReadOnlyMany (ROX), ReadWriteMany (RWX)

**Storage Architecture**
```mermaid
flowchart LR
    Pod["ğŸ“¦ Pod<br/>Needs storage"] --> PVC["ğŸ“‹ PVC<br/>Request: 10GB"]
    PVC --> PV["ğŸ’¾ PV<br/>Cloud disk/NFS<br/>Bound to PVC"]
    
    SC["ğŸ—ï¸ StorageClass<br/>Dynamic provisioner"] --> PV
    
    Cloud["â˜ï¸ Cloud Provider<br/>AWS EBS/GCP PD"] --> PV
    
    style Pod fill:#e8f4f8,stroke:#2c3e50,color:#2c3e50
    style PVC fill:#fef5e7,stroke:#2c3e50,color:#2c3e50
    style PV fill:#f4ecf7,stroke:#2c3e50,color:#2c3e50
    style SC fill:#d5f4e6,stroke:#2c3e50,color:#2c3e50
```

* ğŸ¯ **Access Modes**:
  * **RWO (ReadWriteOnce)**: Single node read-write (most common)
  * **ROX (ReadOnlyMany)**: Multiple nodes read-only
  * **RWX (ReadWriteMany)**: Multiple nodes read-write (NFS, CephFS)

ğŸ”— **Resources:**
* [Kubernetes Storage](https://kubernetes.io/docs/concepts/storage/)
* [Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)

---
